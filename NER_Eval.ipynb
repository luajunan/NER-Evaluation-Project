{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER Eval",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbOTWdvBoGbWRqXDDE2vwQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luajunan/NER-Evaluation-Project/blob/main/NER_Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4kb0T_FD9TdB",
        "outputId": "252f741b-d4cf-4cf5-e2d3-18040b95f1b9"
      },
      "source": [
        "!pip install pypi\n",
        "!pip install polyglot\n",
        "!pip install PyICU\n",
        "!pip install pycld2\n",
        "!pip install Morfessor\n",
        "!polyglot download LANG:zh\n",
        "!polyglot download LANG:en\n",
        "!polyglot download LANG:ms\n",
        "!polyglot download LANG:id\n",
        "!polyglot download embeddings2.en\n",
        "!polyglot download pos2.en\n",
        "!polyglot download embeddings2.id\n",
        "!polyglot download pos2.id\n",
        "!polyglot download embeddings2.ms\n",
        "!polyglot download LANG:vi\n",
        "!polyglot download embeddings2.vi\n",
        "!polyglot download LANG:th\n",
        "!polyglot download embeddings2.th\n",
        "\n",
        "!pip install -U spacy\n",
        "import spacy\n",
        "!python -m spacy download zh_core_web_lg\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import tree2conlltags\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"words\")\n",
        "\n",
        "!pip install deeppavlov\n",
        "import deeppavlov\n",
        "\n",
        "!python -m deeppavlov install squad_bert\n",
        "from deeppavlov import configs, build_model\n",
        "\n",
        "!pip install flair\n",
        "from segtok.segmenter import split_single\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "!pip install tner\n",
        "import tner\n",
        "model = tner.TransformersNER(\"asahi417/tner-xlm-roberta-large-ontonotes5\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypi in /usr/local/lib/python3.7/dist-packages (2.1)\n",
            "Requirement already satisfied: polyglot in /usr/local/lib/python3.7/dist-packages (16.7.4)\n",
            "Requirement already satisfied: PyICU in /usr/local/lib/python3.7/dist-packages (2.7.3)\n",
            "Requirement already satisfied: pycld2 in /usr/local/lib/python3.7/dist-packages (0.41)\n",
            "Requirement already satisfied: Morfessor in /usr/local/lib/python3.7/dist-packages (2.0.6)\n",
            "[polyglot_data] Downloading collection 'LANG:zh'\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]    | Downloading package sgns2.zh to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sgns2.zh is already up-to-date!\n",
            "[polyglot_data]    | Downloading package unipos.zh to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package ner2.zh to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package ner2.zh is already up-to-date!\n",
            "[polyglot_data]    | Downloading package counts2.zh to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package counts2.zh is already up-to-date!\n",
            "[polyglot_data]    | Downloading package transliteration2.zh to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package transliteration2.zh is already up-to-\n",
            "[polyglot_data]    |       date!\n",
            "[polyglot_data]    | Downloading package embeddings2.zh to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package embeddings2.zh is already up-to-date!\n",
            "[polyglot_data]    | Downloading package uniemb.zh to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package sentiment2.zh to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sentiment2.zh is already up-to-date!\n",
            "[polyglot_data]    | Downloading package tsne2.zh to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package tsne2.zh is already up-to-date!\n",
            "[polyglot_data]    | Downloading package morph2.zh to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package morph2.zh is already up-to-date!\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]  Done downloading collection LANG:zh\n",
            "[polyglot_data] Downloading collection 'LANG:en'\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]    | Downloading package sgns2.en to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sgns2.en is already up-to-date!\n",
            "[polyglot_data]    | Downloading package unipos.en to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package ner2.en to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package ner2.en is already up-to-date!\n",
            "[polyglot_data]    | Downloading package counts2.en to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package counts2.en is already up-to-date!\n",
            "[polyglot_data]    | Downloading package embeddings2.en to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package embeddings2.en is already up-to-date!\n",
            "[polyglot_data]    | Downloading package uniemb.en to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package pos2.en to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package pos2.en is already up-to-date!\n",
            "[polyglot_data]    | Downloading package sentiment2.en to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sentiment2.en is already up-to-date!\n",
            "[polyglot_data]    | Downloading package tsne2.en to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package tsne2.en is already up-to-date!\n",
            "[polyglot_data]    | Downloading package morph2.en to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package morph2.en is already up-to-date!\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]  Done downloading collection LANG:en\n",
            "[polyglot_data] Downloading collection 'LANG:ms'\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]    | Downloading package sgns2.ms to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sgns2.ms is already up-to-date!\n",
            "[polyglot_data]    | Downloading package unipos.ms to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package ner2.ms to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package ner2.ms is already up-to-date!\n",
            "[polyglot_data]    | Downloading package counts2.ms to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package counts2.ms is already up-to-date!\n",
            "[polyglot_data]    | Downloading package transliteration2.ms to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package transliteration2.ms is already up-to-\n",
            "[polyglot_data]    |       date!\n",
            "[polyglot_data]    | Downloading package embeddings2.ms to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package embeddings2.ms is already up-to-date!\n",
            "[polyglot_data]    | Downloading package uniemb.ms to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package sentiment2.ms to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sentiment2.ms is already up-to-date!\n",
            "[polyglot_data]    | Downloading package tsne2.ms to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package tsne2.ms is already up-to-date!\n",
            "[polyglot_data]    | Downloading package morph2.ms to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package morph2.ms is already up-to-date!\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]  Done downloading collection LANG:ms\n",
            "[polyglot_data] Downloading collection 'LANG:id'\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]    | Downloading package sgns2.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sgns2.id is already up-to-date!\n",
            "[polyglot_data]    | Downloading package unipos.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package ner2.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package ner2.id is already up-to-date!\n",
            "[polyglot_data]    | Downloading package counts2.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package counts2.id is already up-to-date!\n",
            "[polyglot_data]    | Downloading package transliteration2.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package transliteration2.id is already up-to-\n",
            "[polyglot_data]    |       date!\n",
            "[polyglot_data]    | Downloading package embeddings2.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package embeddings2.id is already up-to-date!\n",
            "[polyglot_data]    | Downloading package uniemb.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package pos2.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package pos2.id is already up-to-date!\n",
            "[polyglot_data]    | Downloading package sentiment2.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sentiment2.id is already up-to-date!\n",
            "[polyglot_data]    | Downloading package tsne2.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package tsne2.id is already up-to-date!\n",
            "[polyglot_data]    | Downloading package morph2.id to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package morph2.id is already up-to-date!\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]  Done downloading collection LANG:id\n",
            "[polyglot_data] Downloading package embeddings2.en to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data]   Package embeddings2.en is already up-to-date!\n",
            "[polyglot_data] Downloading package pos2.en to /root/polyglot_data...\n",
            "[polyglot_data]   Package pos2.en is already up-to-date!\n",
            "[polyglot_data] Downloading package embeddings2.id to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data]   Package embeddings2.id is already up-to-date!\n",
            "[polyglot_data] Downloading package pos2.id to /root/polyglot_data...\n",
            "[polyglot_data]   Package pos2.id is already up-to-date!\n",
            "[polyglot_data] Downloading package embeddings2.ms to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data]   Package embeddings2.ms is already up-to-date!\n",
            "[polyglot_data] Downloading collection 'LANG:vi'\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]    | Downloading package sgns2.vi to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sgns2.vi is already up-to-date!\n",
            "[polyglot_data]    | Downloading package unipos.vi to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package ner2.vi to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package ner2.vi is already up-to-date!\n",
            "[polyglot_data]    | Downloading package counts2.vi to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package counts2.vi is already up-to-date!\n",
            "[polyglot_data]    | Downloading package transliteration2.vi to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package transliteration2.vi is already up-to-\n",
            "[polyglot_data]    |       date!\n",
            "[polyglot_data]    | Downloading package embeddings2.vi to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package embeddings2.vi is already up-to-date!\n",
            "[polyglot_data]    | Downloading package uniemb.vi to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package sentiment2.vi to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sentiment2.vi is already up-to-date!\n",
            "[polyglot_data]    | Downloading package tsne2.vi to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package tsne2.vi is already up-to-date!\n",
            "[polyglot_data]    | Downloading package morph2.vi to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package morph2.vi is already up-to-date!\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]  Done downloading collection LANG:vi\n",
            "[polyglot_data] Downloading package embeddings2.vi to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data]   Package embeddings2.vi is already up-to-date!\n",
            "[polyglot_data] Downloading collection 'LANG:th'\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]    | Downloading package sgns2.th to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sgns2.th is already up-to-date!\n",
            "[polyglot_data]    | Downloading package unipos.th to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package ner2.th to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package ner2.th is already up-to-date!\n",
            "[polyglot_data]    | Downloading package counts2.th to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package counts2.th is already up-to-date!\n",
            "[polyglot_data]    | Downloading package transliteration2.th to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package transliteration2.th is already up-to-\n",
            "[polyglot_data]    |       date!\n",
            "[polyglot_data]    | Downloading package embeddings2.th to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package embeddings2.th is already up-to-date!\n",
            "[polyglot_data]    | Downloading package uniemb.th to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    | Downloading package sentiment2.th to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package sentiment2.th is already up-to-date!\n",
            "[polyglot_data]    | Downloading package tsne2.th to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package tsne2.th is already up-to-date!\n",
            "[polyglot_data]    | Downloading package morph2.th to\n",
            "[polyglot_data]    |     /root/polyglot_data...\n",
            "[polyglot_data]    |   Package morph2.th is already up-to-date!\n",
            "[polyglot_data]    | \n",
            "[polyglot_data]  Done downloading collection LANG:th\n",
            "[polyglot_data] Downloading package embeddings2.th to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data]   Package embeddings2.th is already up-to-date!\n",
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.7/dist-packages (3.0.6)\n",
            "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "  Using cached https://files.pythonhosted.org/packages/ca/fa/d43f31874e1f2a9633e4c025be310f2ce7a8350017579e9e837a62630a7e/pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.22.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.18.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement fastapi==0.47.1, but you'll have fastapi 0.61.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement overrides==2.7.0, but you'll have overrides 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement pydantic==1.3, but you'll have pydantic 1.7.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement scikit-learn==0.21.2, but you'll have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement uvicorn==0.11.7, but you'll have uvicorn 0.11.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pydantic\n",
            "  Found existing installation: pydantic 1.3\n",
            "    Uninstalling pydantic-1.3:\n",
            "      Successfully uninstalled pydantic-1.3\n",
            "Successfully installed pydantic-1.7.4\n",
            "Requirement already satisfied: zh-core-web-lg==3.0.0 from https://github.com/explosion/spacy-models/releases/download/zh_core_web_lg-3.0.0/zh_core_web_lg-3.0.0-py3-none-any.whl#egg=zh_core_web_lg==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from zh-core-web-lg==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-pkuseg<0.1.0,>=0.0.27 in /usr/local/lib/python3.7/dist-packages (from zh-core-web-lg==3.0.0) (0.0.28)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (1.18.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (20.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (56.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (2.22.0)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.7/dist-packages (from spacy-pkuseg<0.1.0,>=0.0.27->zh-core-web-lg==3.0.0) (0.29.14)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->zh-core-web-lg==3.0.0) (3.0.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('zh_core_web_lg')\n",
            "Requirement already satisfied: en-core-web-lg==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl#egg=en_core_web_lg==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (56.1.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.18.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.22.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (20.9)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.4.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deeppavlov in /usr/local/lib/python3.7/dist-packages (0.15.0)\n",
            "Requirement already satisfied: pyopenssl==19.1.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (19.1.0)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.4.417127.4579844)\n",
            "Requirement already satisfied: sacremoses==0.0.35 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.0.35)\n",
            "Requirement already satisfied: ruamel.yaml==0.15.100 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.15.100)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.4.1)\n",
            "Requirement already satisfied: prometheus-client==0.7.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.7.1)\n",
            "Requirement already satisfied: numpy==1.18.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.18.0)\n",
            "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.22.0)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (7.1.2)\n",
            "Requirement already satisfied: pytz==2019.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2019.1)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.10.0)\n",
            "Collecting pydantic==1.3\n",
            "  Using cached https://files.pythonhosted.org/packages/4b/56/1f652c3f658d2a9fd495d2e988a2da57eabdb6c4b8f4563c2ccbe6a2a8c5/pydantic-1.3-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8/overrides-2.7.0-cp37-none-any.whl\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.0.12)\n",
            "Requirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (4.41.1)\n",
            "Requirement already satisfied: Cython==0.29.14 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.29.14)\n",
            "Requirement already satisfied: uvloop==0.14.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.14.0)\n",
            "Collecting uvicorn==0.11.7\n",
            "  Using cached https://files.pythonhosted.org/packages/a9/5f/2bc87272f189662e129ddcd4807ad3ef83128b4df3a3482335f5f9790f24/uvicorn-0.11.7-py3-none-any.whl\n",
            "Collecting fastapi==0.47.1\n",
            "  Using cached https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.4.5)\n",
            "Collecting scikit-learn==0.21.2\n",
            "  Using cached https://files.pythonhosted.org/packages/21/a4/a48bd4b0d15395362b561df7e7247de87291105eb736a3b2aaffebf437b9/scikit_learn-0.21.2-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: aio-pika==6.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (6.4.1)\n",
            "Requirement already satisfied: pandas==0.25.3 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.25.3)\n",
            "Requirement already satisfied: rusenttokenize==0.0.5 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.0.5)\n",
            "Requirement already satisfied: pymorphy2==0.8 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.8)\n",
            "Requirement already satisfied: pytelegrambotapi==3.6.7 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.6.7)\n",
            "Requirement already satisfied: cryptography>=2.8 in /usr/local/lib/python3.7/dist-packages (from pyopenssl==19.1.0->deeppavlov) (3.4.7)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyopenssl==19.1.0->deeppavlov) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->deeppavlov) (1.0.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2020.12.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.7->deeppavlov) (0.9.0)\n",
            "Requirement already satisfied: websockets==8.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.7->deeppavlov) (8.1)\n",
            "Requirement already satisfied: httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.7->deeppavlov) (0.1.2)\n",
            "Processing /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5/starlette-0.12.9-cp37-none-any.whl\n",
            "Requirement already satisfied: yarl in /usr/local/lib/python3.7/dist-packages (from aio-pika==6.4.1->deeppavlov) (1.6.3)\n",
            "Requirement already satisfied: aiormq<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from aio-pika==6.4.1->deeppavlov) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (3.7.4.3)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (5.1.0)\n",
            "Requirement already satisfied: pamqp==2.3.0 in /usr/local/lib/python3.7/dist-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
            "\u001b[31mERROR: tner 0.0.8 has requirement fastapi==0.61.0, but you'll have fastapi 0.47.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tner 0.0.8 has requirement uvicorn==0.11.8, but you'll have uvicorn 0.11.7 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: thinc 8.0.3 has requirement pydantic<1.8.0,>=1.7.1, but you'll have pydantic 1.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 3.0.6 has requirement pydantic<1.8.0,>=1.7.1, but you'll have pydantic 1.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seqeval 1.2.2 has requirement scikit-learn>=0.21.3, but you'll have scikit-learn 0.21.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.5 has requirement overrides<4.0.0,>=3.0.0, but you'll have overrides 2.7.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.5 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.18.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: flair 0.8.0.post1 has requirement scikit-learn>=0.21.3, but you'll have scikit-learn 0.21.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pydantic, overrides, uvicorn, starlette, fastapi, scikit-learn\n",
            "  Found existing installation: pydantic 1.7.4\n",
            "    Uninstalling pydantic-1.7.4:\n",
            "      Successfully uninstalled pydantic-1.7.4\n",
            "  Found existing installation: overrides 3.1.0\n",
            "    Uninstalling overrides-3.1.0:\n",
            "      Successfully uninstalled overrides-3.1.0\n",
            "  Found existing installation: uvicorn 0.11.8\n",
            "    Uninstalling uvicorn-0.11.8:\n",
            "      Successfully uninstalled uvicorn-0.11.8\n",
            "  Found existing installation: starlette 0.13.6\n",
            "    Uninstalling starlette-0.13.6:\n",
            "      Successfully uninstalled starlette-0.13.6\n",
            "  Found existing installation: fastapi 0.61.0\n",
            "    Uninstalling fastapi-0.61.0:\n",
            "      Successfully uninstalled fastapi-0.61.0\n",
            "  Found existing installation: scikit-learn 0.24.2\n",
            "    Uninstalling scikit-learn-0.24.2:\n",
            "      Successfully uninstalled scikit-learn-0.24.2\n",
            "Successfully installed fastapi-0.47.1 overrides-2.7.0 pydantic-1.3 scikit-learn-0.21.2 starlette-0.12.9 uvicorn-0.11.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydantic",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:46:43.198 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'squad_bert' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/squad/squad_bert.json'\n",
            "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
            "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-nku04jb1\n",
            "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-nku04jb1\n",
            "Requirement already satisfied (use --upgrade to upgrade): bert-dp==1.0 from git+https://github.com/deepmipt/bert.git@feat/multi_gpu in /usr/local/lib/python3.7/dist-packages\n",
            "Building wheels for collected packages: bert-dp\n",
            "  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-dp: filename=bert_dp-1.0-cp37-none-any.whl size=23581 sha256=d45bfc17af3ed674c46024160b6ff01edcbeb1b0e9050784ce2c7e7ce452d5bd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0fkj8jxt/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n",
            "Successfully built bert-dp\n",
            "Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.7/dist-packages (1.15.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.34.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.12.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (56.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.7/dist-packages (0.8.0.post1)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.3.1)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from flair) (1.2.12)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: torch<=1.7.1,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.7.1)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.6.5)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from flair) (0.3.3)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from flair) (6.0.3)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.7/dist-packages (from flair) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from flair) (0.0.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.6.1)\n",
            "Collecting scikit-learn>=0.21.3\n",
            "  Using cached https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.7/dist-packages (from flair) (1.5.10)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.95)\n",
            "Requirement already satisfied: gdown==3.12.2 in /usr/local/lib/python3.7/dist-packages (from flair) (3.12.2)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.7.0)\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.18.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect->flair) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2020.12.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (7.1.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (3.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.7.1,>=1.5.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
            "Processing /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9/overrides-3.1.0-cp37-none-any.whl\n",
            "Collecting requests<3.0.0,>=2.25.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.0.35)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (20.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (2.1.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tner 0.0.8 has requirement fastapi==0.61.0, but you'll have fastapi 0.47.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tner 0.0.8 has requirement uvicorn==0.11.8, but you'll have uvicorn 0.11.7 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 3.0.6 has requirement pydantic<1.8.0,>=1.7.1, but you'll have pydantic 1.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.18.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement overrides==2.7.0, but you'll have overrides 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement requests==2.22.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement scikit-learn==0.21.2, but you'll have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn, overrides, requests\n",
            "  Found existing installation: scikit-learn 0.21.2\n",
            "    Uninstalling scikit-learn-0.21.2:\n",
            "      Successfully uninstalled scikit-learn-0.21.2\n",
            "  Found existing installation: overrides 2.7.0\n",
            "    Uninstalling overrides-2.7.0:\n",
            "      Successfully uninstalled overrides-2.7.0\n",
            "  Found existing installation: requests 2.22.0\n",
            "    Uninstalling requests-2.22.0:\n",
            "      Successfully uninstalled requests-2.22.0\n",
            "Successfully installed overrides-3.1.0 requests-2.25.1 scikit-learn-0.24.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tner in /usr/local/lib/python3.7/dist-packages (0.0.8)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tner) (0.10.2)\n",
            "Requirement already satisfied: jinja2==2.11.3 in /usr/local/lib/python3.7/dist-packages (from tner) (2.11.3)\n",
            "Requirement already satisfied: aiofiles==0.5.0 in /usr/local/lib/python3.7/dist-packages (from tner) (0.5.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from tner) (4.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from tner) (1.7.1)\n",
            "Requirement already satisfied: sudachipy in /usr/local/lib/python3.7/dist-packages (from tner) (0.5.2)\n",
            "Requirement already satisfied: sudachidict-core in /usr/local/lib/python3.7/dist-packages (from tner) (20201223.post1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from tner) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from tner) (0.1.95)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from tner) (1.2.2)\n",
            "Collecting uvicorn==0.11.8\n",
            "  Using cached https://files.pythonhosted.org/packages/32/9a/5f619c02f36e751071c2b7eaa37a7c4b767feb41e4c2de48e8fbe4e7b451/uvicorn-0.11.8-py3-none-any.whl\n",
            "Collecting fastapi==0.61.0\n",
            "  Using cached https://files.pythonhosted.org/packages/82/cb/96cb7cc6a807af493f0083e7d854fdd568ae5335f8f93b96c966fabd8d2f/fastapi-0.61.0-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib==3.3.1 in /usr/local/lib/python3.7/dist-packages (from tner) (3.3.1)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.7/dist-packages (from tner) (1.5.10)\n",
            "Requirement already satisfied: Pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from tner) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2==2.11.3->tner) (2.0.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->tner) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->tner) (0.0.35)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->tner) (1.18.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->tner) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->tner) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers->tner) (3.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->tner) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->tner) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers->tner) (0.0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->tner) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->tner) (3.7.4.3)\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.7/dist-packages (from sudachipy->tner) (2.1.0)\n",
            "Requirement already satisfied: dartsclone~=0.9.0 in /usr/local/lib/python3.7/dist-packages (from sudachipy->tner) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->tner) (3.3.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->tner) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->tner) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->tner) (0.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->tner) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->tner) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard->tner) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->tner) (1.34.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tner) (0.24.2)\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.8->tner) (7.1.2)\n",
            "Requirement already satisfied: uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.8->tner) (0.14.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.8->tner) (0.9.0)\n",
            "Requirement already satisfied: httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.8->tner) (0.1.2)\n",
            "Requirement already satisfied: websockets==8.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.8->tner) (8.1)\n",
            "Collecting starlette==0.13.6\n",
            "  Using cached https://files.pythonhosted.org/packages/c5/a4/c9e228d7d47044ce4c83ba002f28ff479e542455f0499198a3f77c94f564/starlette-0.13.6-py3-none-any.whl\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastapi==0.61.0->tner) (1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->tner) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->tner) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->tner) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->tner) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->tner) (2020.12.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->tner) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->tner) (3.4.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->tner) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->tner) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->tner) (1.24.3)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from dartsclone~=0.9.0->sudachipy->tner) (0.29.14)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tner) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tner) (2.1.0)\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement fastapi==0.47.1, but you'll have fastapi 0.61.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement overrides==2.7.0, but you'll have overrides 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement requests==2.22.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement scikit-learn==0.21.2, but you'll have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.15.0 has requirement uvicorn==0.11.7, but you'll have uvicorn 0.11.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: uvicorn, starlette, fastapi\n",
            "  Found existing installation: uvicorn 0.11.7\n",
            "    Uninstalling uvicorn-0.11.7:\n",
            "      Successfully uninstalled uvicorn-0.11.7\n",
            "  Found existing installation: starlette 0.12.9\n",
            "    Uninstalling starlette-0.12.9:\n",
            "      Successfully uninstalled starlette-0.12.9\n",
            "  Found existing installation: fastapi 0.47.1\n",
            "    Uninstalling fastapi-0.47.1:\n",
            "      Successfully uninstalled fastapi-0.47.1\n",
            "Successfully installed fastapi-0.61.0 starlette-0.13.6 uvicorn-0.11.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:47:09 INFO     *** initialize network ***\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGsTuqSjLYF6"
      },
      "source": [
        "import polyglot as poly\n",
        "from polyglot.text import Text, Word\n",
        "\n",
        "import spacy\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import tree2conlltags\n",
        "\n",
        "import deeppavlov\n",
        "from deeppavlov import configs, build_model\n",
        "\n",
        "from segtok.segmenter import split_single\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "import tner\n",
        "\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0JAoGFNHRHw",
        "outputId": "75f5d53e-e4ab-4d8e-a4f7-3b4210059b57"
      },
      "source": [
        "ner_model = build_model(configs.ner.ner_ontonotes_bert_mult, download=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:14.137 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip download because of matching hashes\n",
            "2021-05-31 05:48:14 INFO     Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip download because of matching hashes\n",
            "2021-05-31 05:48:41.355 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/ner_ontonotes_bert_mult_v1.tar.gz download because of matching hashes\n",
            "2021-05-31 05:48:41 INFO     Skipped http://files.deeppavlov.ai/deeppavlov_data/ner_ontonotes_bert_mult_v1.tar.gz download because of matching hashes\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Package perluniprops is already up-to-date!\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
            "2021-05-31 05:48:41 WARNING  From /usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "2021-05-31 05:48:41.853 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_ontonotes_bert_mult/tag.dict]\n",
            "2021-05-31 05:48:41 INFO     [loading vocabulary from /root/.deeppavlov/models/ner_ontonotes_bert_mult/tag.dict]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:41 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:41 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:41 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:41 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:236: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:41 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:236: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:314: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:41 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:314: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:42 WARNING  From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:42 WARNING  From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:42 WARNING  From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:42 WARNING  \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "2021-05-31 05:48:42 INFO     NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:43 WARNING  From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:43 WARNING  From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:43 WARNING  From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:43 WARNING  From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:47 WARNING  Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:75: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:47 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:75: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:48 WARNING  From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:48 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:48 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:48:48 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:94: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:49:00 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:94: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:49:00 WARNING  From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:671: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:49:04 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:671: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:49:04 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:249: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:49:22 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:249: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2021-05-31 05:49:22.723 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/ner_ontonotes_bert_mult/model]\n",
            "2021-05-31 05:49:22 INFO     [loading model from /root/.deeppavlov/models/ner_ontonotes_bert_mult/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:49:22 WARNING  From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/ner_ontonotes_bert_mult/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-31 05:49:22 INFO     Restoring parameters from /root/.deeppavlov/models/ner_ontonotes_bert_mult/model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMp_SAxnEVIb"
      },
      "source": [
        "def read_conll_file(filename):\n",
        "  para = \"\"\n",
        "  actual = []\n",
        "  with open(file= filename) as f:\n",
        "    for line in f:\n",
        "      l = line.split()\n",
        "      if len(l) >= 2 and l[0] != '-DOCSTART-':\n",
        "        puncs = [\",\", \".\", \",\", \"'\", \"`\"]\n",
        "        any_punc = any(punc in l[0][0] for punc in puncs)\n",
        "        ner_tag = l[-1].upper()\n",
        "        if len(ner_tag) > 5:\n",
        "          ner_tag = ner_tag[:5]\n",
        "        if any_punc:\n",
        "          para += ' '.join(l[:len(l)-1])\n",
        "        else:\n",
        "          para += \" \" + ' '.join(l[:len(l)-1])\n",
        "        l = [l[0], l[1]]\n",
        "        actual.append(l)\n",
        "  return [para, actual]\n",
        "\n",
        "def read_file(filename):\n",
        "  para = \"\"\n",
        "  with open(file= filename) as f:\n",
        "    lines = f.read().splitlines()\n",
        "    for l in lines:\n",
        "      para += l\n",
        "  return para\n",
        "\n",
        "def clean_para(txt):\n",
        "  m1 = re.sub(r'(?<=<)([\\w\\s]+)(?=>)|(?<=<\\/)([\\w\\s]+)(?=>)', \"\",txt)\n",
        "  m2 = re.sub(r'[<]|[/]|[>]', \"\",m1)\n",
        "  return m2\n",
        "\n",
        "def para_to_conll(text):\n",
        "\n",
        "  actual_dic = {}\n",
        "  ent_list = []\n",
        "\n",
        "  ent = re.findall(r'(?<=>)([\\w\\s]+)(?=<\\/)', text)\n",
        "  ner = re.findall(r'(?<=<)([\\w\\s]+)(?=>)', text)\n",
        "\n",
        "  for i in ent:\n",
        "    split = i.split()\n",
        "    ent_list += [split]\n",
        "\n",
        "  for j in range(len(ent_list)):\n",
        "    for k in range(len(ent_list[j])):\n",
        "      if k == 0:\n",
        "        ner_tag = \"B\" + \"-\" + ner[j][:3]\n",
        "        if ner_tag not in actual_dic.keys():\n",
        "          actual_dic[ner_tag] = []\n",
        "        actual_dic[ner_tag].append(ent_list[j][k])\n",
        "      else:\n",
        "        ner_tag = \"I\" + \"-\" + ner[j][:3]\n",
        "        if ner_tag not in actual_dic.keys():\n",
        "          actual_dic[ner_tag] = []\n",
        "        actual_dic[ner_tag].append(ent_list[j][k])\n",
        "\n",
        "  return actual_dic"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx63HRJLO9LT"
      },
      "source": [
        "#Takes in the actual NER, model's NER and the entities flagged by the model\n",
        "def tabulate_score(actual_dic, model_dic, model_ent, model_name):\n",
        "\n",
        "  #removing entities that are not captured by the model\n",
        "  actual_dic_score = {k: v for k, v in actual_dic.items() if k in model_dic.keys()}\n",
        "  \n",
        "  clean_model_dic = {}\n",
        "  for k, v in model_dic.items():\n",
        "    if k in actual_dic.keys():\n",
        "      clean_model_dic[k] = v\n",
        "\n",
        "  TP = 0\n",
        "  FN = 0\n",
        "  FP = 0\n",
        "  \n",
        "  for k, v in actual_dic_score.items():\n",
        "    if k in clean_model_dic.keys():\n",
        "      actual_set = set(v)\n",
        "      test_set = set(clean_model_dic[k])\n",
        "      TP += len(actual_set.intersection(test_set))\n",
        "\n",
        "  for k, v in actual_dic_score.items():\n",
        "    for e in v:\n",
        "      if e not in model_ent:\n",
        "        FN += 1\n",
        "\n",
        "  for k1, v1 in actual_dic_score.items():\n",
        "    for k2, v2 in clean_model_dic.items():\n",
        "      if k1 == k2:\n",
        "        FP += len(set(v2)) - len(set(v1).intersection(set(v2)))\n",
        "      #if k1 != k2:\n",
        "        #FP += len(set(v1).intersection(set(v2)))\n",
        "  f1 = round(TP/(TP + 0.5*(FN+FP)), 4)\n",
        "  recall = round(TP/(TP + FN), 4)\n",
        "  precision = round(TP/(TP + FP), 4)\n",
        "  print(f\"F1/Recall/Precision scores with {model_name}\")\n",
        "  print(f\"F1:\", f1, f\"| Recall:\", recall, f\"| Precision:\", precision)\n",
        "  return [f1,recall,precision, model_name]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tTADgSrA8Lj"
      },
      "source": [
        "def polyglot(text, language_code):\n",
        "  dic = {}\n",
        "  ent = []\n",
        "\n",
        "  if language_code == 'zh':\n",
        "    text = re.sub(\" \", \"\",text)\n",
        "\n",
        "  t = Text(text, hint_language_code=language_code)\n",
        "  for sent in t.sentences:\n",
        "    for entity in sent.entities:\n",
        "      if entity.tag not in dic:\n",
        "        dic[entity.tag] = []\n",
        "      for e in entity:\n",
        "        dic[entity.tag].append(e)\n",
        "        ent.append(e)\n",
        "\n",
        "  #For Chinese\n",
        "  if language_code == 'zh':\n",
        "    final_dic = {}\n",
        "    ent = []\n",
        "    for k, v in dic.items():\n",
        "      for word in v:\n",
        "        for i in range(len(word)):\n",
        "          if i == 0:\n",
        "            ner_tag = 'B' + k[1:]\n",
        "          else:\n",
        "            ner_tag = 'I' + k[1:]\n",
        "          if ner_tag not in final_dic.keys():\n",
        "            final_dic[ner_tag] = []\n",
        "          final_dic[ner_tag].append(word[i])\n",
        "          ent.append(word[i])\n",
        "    dic = final_dic\n",
        "\n",
        "  #return dictionary of key as labels, values as a list of all the associated words and list of entities\n",
        "  return [dic, ent]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf7USECDBAL0"
      },
      "source": [
        "def spa(text, language_code):\n",
        "\n",
        "  dic = {}\n",
        "  ent = []\n",
        "  \n",
        "  if language_code == \"en\":\n",
        "    #remove everything except NER\n",
        "    nlp = spacy.load(\"en_core_web_lg\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
        "    t = nlp(text)\n",
        "    \n",
        "  else:\n",
        "    nlp = spacy.load(\"zh_core_web_lg\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
        "    text = re.sub(\" \", \"\",text)\n",
        "    t = nlp(text)\n",
        "\n",
        "  for token in t:\n",
        "    if (token.ent_type_):\n",
        "      ner = token.ent_iob_ + \"-\" + token.ent_type_\n",
        "\n",
        "      if len(ner) > 4:\n",
        "        ner = ner[:5]\n",
        "        if ner not in dic.keys():\n",
        "          dic[ner] = []\n",
        "        dic[ner].append(token.text)\n",
        "      else:\n",
        "        if ner not in dic.keys():\n",
        "          dic[ner] = []\n",
        "        dic[ner].append(token.text)\n",
        "      ent.append(token.text)\n",
        "\n",
        "  if language_code == 'zh':\n",
        "    final_dic = {}\n",
        "    ent = []\n",
        "    for k, v in dic.items():\n",
        "      for word in v:\n",
        "        for i in range(len(word)):\n",
        "          if i == 0:\n",
        "            ner_tag = 'B' + k[1:]\n",
        "          else:\n",
        "            ner_tag = 'I' + k[1:]\n",
        "          if ner_tag not in final_dic.keys():\n",
        "            final_dic[ner_tag] = []\n",
        "          final_dic[ner_tag].append(word[i])\n",
        "          ent.append(word[i])\n",
        "    dic = final_dic \n",
        "  return [dic, ent]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6xgBBovBC0M"
      },
      "source": [
        "def extract_ne_nltk(text):\n",
        "    dic = {}\n",
        "    ent = []\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    tags = nltk.pos_tag(words)\n",
        "    chunks = nltk.ne_chunk(tags)\n",
        "\n",
        "    for chunk in tree2conlltags(chunks):\n",
        "      ner_tag = chunk[2]\n",
        "      text = chunk[0]\n",
        "      if ner_tag not in dic.keys():\n",
        "        if len(ner_tag) > 5:\n",
        "          ner_tag = ner_tag[:5]\n",
        "        dic[ner_tag] = []\n",
        "      dic[ner_tag].append(text)\n",
        "      ent.append(text)\n",
        "    return [dic, ent]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvUhiokTBPsD"
      },
      "source": [
        "def extract_ne_deeppavlov(text, space=True):\n",
        "\n",
        "  para_list = []\n",
        "  it = 0\n",
        "  sent = \"\"\n",
        "  if space:\n",
        "    for i in text.split():\n",
        "      len_token = len(i)\n",
        "      if it+len_token >= 512:\n",
        "        para_list.append([sent])\n",
        "        sent = \"\"\n",
        "        it = 0\n",
        "      else:\n",
        "        sent += \" \" + i\n",
        "        it += len_token + 1\n",
        "  else:\n",
        "    for i in text.split():\n",
        "      len_token = len(i)\n",
        "      if it+len_token >= 512:\n",
        "        para_list.append([sent])\n",
        "        sent = \"\"\n",
        "        it = 0\n",
        "      else:\n",
        "        sent += i\n",
        "        it += len_token + 1\n",
        "\n",
        "  ent = []\n",
        "  ner = []\n",
        "\n",
        "  for sent in para_list:\n",
        "    temp_ent, temp_ner = ner_model(sent)\n",
        "    ent += temp_ent\n",
        "    ner += temp_ner\n",
        "\n",
        "  ent_list = [item for sublist in ent for item in sublist]\n",
        "\n",
        "  dic = {}\n",
        "\n",
        "  for i in range(len(ner)):\n",
        "    for j in range(len(ner[i])):\n",
        "      ner_tag = ner[i][j]\n",
        "      if len(ner_tag) > 4:\n",
        "        ner_tag = ner_tag[:5]\n",
        "        if ner_tag not in dic.keys():\n",
        "          dic[ner_tag] = []\n",
        "        dic[ner_tag].append(ent[i][j])\n",
        "      else:\n",
        "        if ner_tag not in dic.keys():\n",
        "          dic[ner_tag] = []\n",
        "        dic[ner_tag].append(ent[i][j])\n",
        "\n",
        "  if not space:\n",
        "      final_dic = {}\n",
        "      ent = []\n",
        "      for k, v in dic.items():\n",
        "        for word in v:\n",
        "          for i in range(len(word)):\n",
        "            if i == 0:\n",
        "              ner_tag = 'B' + k[1:]\n",
        "            else:\n",
        "              ner_tag = 'I' + k[1:]\n",
        "            if ner_tag not in final_dic.keys():\n",
        "              final_dic[ner_tag] = []\n",
        "            final_dic[ner_tag].append(word[i])\n",
        "            ent.append(word[i])\n",
        "      dic = final_dic\n",
        "      ent_list = ent\n",
        "\n",
        "  return [dic, ent_list]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUxvTQuMBRmT"
      },
      "source": [
        "def extract_ne_flair(text):\n",
        "\n",
        "  tokens_list = []\n",
        "  ner_list = []\n",
        "  dic = {}\n",
        "\n",
        "  # make a sentence\n",
        "  sentences = [Sentence(sent, use_tokenizer=True) for sent in split_single(text)]\n",
        "  # load the NER tagger\n",
        "  tagger = SequenceTagger.load('ner')\n",
        "  # run NER over sentence\n",
        "  tagger.predict(sentences)\n",
        "  for sent in sentences:\n",
        "    for tokens in sent:\n",
        "      tokens_list.append(tokens.text)\n",
        "      ner_tag = tokens.get_labels()[0].value\n",
        "      ner_list.append(ner_tag)\n",
        "\n",
        "  for i in range(len(ner_list)):\n",
        "    if ner_list[i] not in dic.keys():\n",
        "      dic[ner_list[i]] = []\n",
        "    dic[ner_list[i]].append(tokens_list[i])\n",
        "\n",
        "  return [dic, tokens_list]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xQZg9sIBYRA"
      },
      "source": [
        "def extract_ne_roberta(text, space=True):\n",
        "  para_list = []\n",
        "  it = 0\n",
        "  sent = \"\"\n",
        "  if space:\n",
        "    for i in text.split():\n",
        "      len_token = len(i)\n",
        "      if it+len_token >= 512:\n",
        "        para_list.append([sent])\n",
        "        sent = \"\"\n",
        "        it = 0\n",
        "      else:\n",
        "        sent += \" \" + i\n",
        "        it += len_token + 1\n",
        "  else:\n",
        "    for i in text.split():\n",
        "      len_token = len(i)\n",
        "      if it+len_token >= 512:\n",
        "        para_list.append([sent])\n",
        "        sent = \"\"\n",
        "        it = 0\n",
        "      else:\n",
        "        sent += i\n",
        "        it += len_token\n",
        "\n",
        "  dic = {}\n",
        "  for s in para_list:\n",
        "    pred = model.predict(s)\n",
        "    l = pred[0]['entity']\n",
        "    if l:\n",
        "      for i in range(len(l)):\n",
        "        tag = l[i]['type'][:3].upper()\n",
        "        if tag not in dic.keys():\n",
        "          dic[tag] = []\n",
        "        ent = l[i]['mention']\n",
        "        dic[tag].append(ent)\n",
        "\n",
        "  model_dic_roberta = {}\n",
        "  model_ent_roberta = []\n",
        "  for k, v in dic.items():\n",
        "    for ent in v:\n",
        "      for i in range(len(ent.split())):\n",
        "        if i == 0:\n",
        "          ner_tag = \"B-\" + k\n",
        "          if ner_tag not in model_dic_roberta.keys():\n",
        "            model_dic_roberta[ner_tag] = []\n",
        "          model_dic_roberta[ner_tag].append(ent.split()[i])\n",
        "        else:\n",
        "          ner_tag = \"I-\" + k\n",
        "          if ner_tag not in model_dic_roberta.keys():\n",
        "            model_dic_roberta[ner_tag] = []\n",
        "          model_dic_roberta[ner_tag].append(ent.split()[i])\n",
        "        model_ent_roberta.append(ent.split()[i])\n",
        "  \n",
        "  dic = model_dic_roberta\n",
        "  ent = model_ent_roberta\n",
        "\n",
        "  if not space:\n",
        "    final_dic = {}\n",
        "    ent = []\n",
        "    for k, v in dic.items():\n",
        "      for word in v:\n",
        "        for i in range(len(word)):\n",
        "          if i == 0:\n",
        "            ner_tag = 'B' + k[1:]\n",
        "          else:\n",
        "            ner_tag = 'I' + k[1:]\n",
        "          if ner_tag not in final_dic.keys():\n",
        "            final_dic[ner_tag] = []\n",
        "          final_dic[ner_tag].append(word[i])\n",
        "          ent.append(word[i])\n",
        "    dic = final_dic \n",
        "\n",
        "        \n",
        "  return [dic, ent]\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKoa23KnESA0"
      },
      "source": [
        "def main(filename, language_code, file_type):\n",
        "  results_list = []\n",
        "  actual_dic = {}\n",
        "\n",
        "  if file_type == \"conll\":\n",
        "    para, actual = read_conll_file(filename)\n",
        "\n",
        "    actual_dic = {}\n",
        "    for i in range(len(actual)):\n",
        "      entity = actual[i][-1].upper()\n",
        "      sep = \" \"\n",
        "      text = sep.join(actual[i][:len(actual[i])-1])\n",
        "      if len(entity) > 4:\n",
        "        entity = entity[:5]\n",
        "      if entity not in actual_dic.keys():\n",
        "        actual_dic[entity] = []\n",
        "        actual_dic[entity].append(text)\n",
        "      else:\n",
        "        actual_dic[entity].append(text)\n",
        "\n",
        "  elif file_type == \"text\":\n",
        "    txt = read_file(filename)\n",
        "    para = clean_para(txt)\n",
        "    actual_dic = para_to_conll(txt)\n",
        "\n",
        "  else:\n",
        "    print('Error: File type not supported. Only conll or text files are supported.')\n",
        "    return\n",
        "\n",
        "  if language_code == \"en\":\n",
        "    model_dic_poly, model_ent_poly = polyglot(para, language_code)\n",
        "    model_dic_spa, model_ent_spa = spa(para, language_code)\n",
        "    model_dic_nltk, model_ent_nltk = extract_ne_nltk(para)\n",
        "    model_dic_deeppavlov, model_ent_deeppavlov = extract_ne_deeppavlov(para)\n",
        "    model_dic_flair, model_ent_flair = extract_ne_flair(para)\n",
        "    model_dic_roberta, model_ent_roberta = extract_ne_roberta(para)\n",
        "\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_poly, model_ent_poly, \"Polyglot\"))\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_spa, model_ent_spa, \"Spacy\"))\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_nltk, model_ent_nltk, \"NLTK\"))\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_deeppavlov, model_ent_deeppavlov, \"Deeppavlov\"))\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_flair, model_ent_flair, \"Flair\"))\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_roberta, model_ent_roberta, \"Roberta\"))\n",
        "\n",
        "  #Running each model\n",
        "\n",
        "  elif language_code == \"ms\" or language_code == \"id\" or language_code == 'vi' or language_code == 'th':\n",
        "    model_dic_poly, model_ent_poly = polyglot(para, language_code)\n",
        "    model_dic_deeppavlov, model_ent_deeppavlov = extract_ne_deeppavlov(para)\n",
        "    model_dic_roberta, model_ent_roberta = extract_ne_roberta(para)\n",
        "\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_poly, model_ent_poly, \"Polyglot\"))\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_deeppavlov, model_ent_deeppavlov, \"Deeppavlov\"))\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_roberta, model_ent_roberta, \"Roberta\"))\n",
        "\n",
        "  elif language_code == \"zh\":\n",
        "    space=False\n",
        "    model_dic_poly, model_ent_poly = polyglot(para, language_code)\n",
        "    model_dic_spa, model_ent_spa = spa(para, language_code)\n",
        "    model_dic_deeppavlov, model_ent_deeppavlov = extract_ne_deeppavlov(para, space)\n",
        "    model_dic_roberta, model_ent_roberta = extract_ne_roberta(para, space)\n",
        "\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_poly, model_ent_poly, \"Polyglot\"))\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_spa, model_ent_spa, \"Spacy\"))\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_deeppavlov, model_ent_deeppavlov, \"Deeppavlov\"))\n",
        "    results_list.append(tabulate_score(actual_dic, model_dic_roberta, model_ent_roberta, \"Roberta\"))\n",
        "\n",
        "  else:\n",
        "    print(\"Language code not supported. Languages supported include en, ms, id, zh and th.\")\n",
        "\n",
        "  return results_list"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}